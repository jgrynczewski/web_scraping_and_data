{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65e46dea-64d3-419a-b8b0-944a1fb4cc7a",
   "metadata": {},
   "source": [
    "`Co może pójść nie tak?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf71ab6-cff4-44b9-a59d-f5f5ffb50401",
   "metadata": {},
   "source": [
    "### 1. Monitorowanie nagłówków"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4198e53-bc76-4983-9719-576bd17cfa27",
   "metadata": {},
   "source": [
    "Strony na różne sposoby są zabezpieczane przed scrapingiem. Najprostszym sposobem zabezpieczenia przed scrapingiem jest sprawdzanie nagłówków zapytania. W nagłówkach zapytania jest pole `User-Agent`, które przeglądarki wypełniają podając w nim swoje podstawowe parametry. Strony często sprawdzają, czy nagłówek `User-Agent` jest sensowny i jeżeli nie jest to odrzucają zapytanie. Jako przykład weźmy stronę Narodowego Banku Polskiego [NBP](https://www.nbp.pl/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d92241e-fd0d-4938-b2ae-3863ba1a758b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T08:25:52.424609Z",
     "start_time": "2024-12-17T08:25:51.863602Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<html>\\r\\n<head>\\r\\n<META NAME=\"robots\" CONTENT=\"noindex,nofollow\">\\r\\n<script src=\"/_Incapsula_Resource?SWJIYLWA=5074a744e2e3d891814e9a2dace20bd4,719d34d31c8e3a6e6fffd425f7e032f3\">\\r\\n</script>\\r\\n<body>\\r\\n</body></html>\\r\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://nbp.pl/\"\n",
    "response = requests.get(url)\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f35fce-aa4b-4fa9-8a37-3f698356c9d4",
   "metadata": {},
   "source": [
    "Jakie nagłówki wysłaliśmy w zapytaniu?\n",
    "\n",
    "Żeby to sprawdzić nie możemy posługiwać się metodą `get`, która tworzy i natychmiast wysyła zapytanie. Stwórzmy identyczne zapytanie ręcznie, przy pomocy klas `Session` i `Request`. W ten sposób będziemy mogli zobaczyć domyślne nagłówki, które biblioteka `requests` umieszcza w zapytaniu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd95aa09-d3bb-4174-82af-fa3d634696b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'User-Agent': 'python-requests/2.32.3', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session = requests.Session()\n",
    "request = requests.Request('GET', url)\n",
    "\n",
    "prepared_request = session.prepare_request(request)\n",
    "prepared_request.headers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27940bd5-e221-4d32-9991-e387bef9542d",
   "metadata": {},
   "source": [
    "Popatrzmy w przeglądarce jakie nagłówki dodaje.\n",
    "\n",
    "Ten, na którym nam obecnie zależy to:\n",
    "\n",
    "User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0\n",
    "\n",
    "Co on oznacza? \\\n",
    "Sprawdźmy https://useragentstring.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6065ac-92ed-496c-8c28-20fd714176dc",
   "metadata": {},
   "source": [
    "Ustawmy go w naszym zapytaniu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b87f3ce8-d719-46e2-a907-9e42237a0577",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://nbp.pl/\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0\",\n",
    "    \"sec-ch-ua\": '\"Microsoft Edge\";v=\"131\", \"Chromium\";v=\"131\", \"Not_A Brand\";v=\"24\"'\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "content = response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21787be-3798-469f-8087-90411eb83aaa",
   "metadata": {},
   "source": [
    "Tym razem dostalismy prawidłową odpowiedź. Wyciągnijmy z niej kursy walut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55ea29fd-da8a-44d0-8e3f-836a95986b4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 USD: 4,0571\n",
      "1 EUR: 4,2622\n",
      "1 CHF: 4,5563\n",
      "1 GBP: 5,1339\n",
      "100 JPY: 2,6379\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(content, 'html.parser')\n",
    "currencies = soup.find_all(class_=\"exchange__table-col-1\")\n",
    "rates = soup.find_all(class_=\"exchange__table-col-2\")\n",
    "\n",
    "for currency, rate in zip(currencies, rates):\n",
    "    currency = currency.text.strip()\n",
    "    rate = rate.text.strip()\n",
    "    print(f\"{currency}: {rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19690ce-2586-417d-991d-9258c635ce83",
   "metadata": {},
   "source": [
    "Jak to wygląda od strony prawnej?\n",
    "\n",
    "Jeżeli dane są publicznie dostępne i nie zawierają wrażliwych informacji, to ich scraping jest legalny. W przypadku informacji, które uzyskać możemy dopiero po zalogowaniu się do określonego portalu ich scraping jest nielegalny. W tym przypadku najlepiej poszukać szczegółowych informacji na stronie portalu, ale w przeważającej liczbie przypadków scraping danych niepublicznych (do których uzyskania trzeba być uwierzytelnionym) jest nielegalny."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33cfe36-1a3c-4af6-92e2-a19d079f0cba",
   "metadata": {},
   "source": [
    "### 2. Monitorowanie IP\n",
    "\n",
    "Standardowo monitorowana jest też liczba zapytań przychodzących z jednego ip w jednoste czasu. Jeżeli zapytania przychodzą szybciej niż jakikolwiek człowiek byłby w stanie wykonać ip często jest blokowany na tej stronie. Istnieją dwa popoularne sposoby na obchodzenie tego mechanizmu:\n",
    "- dorzucanie `time.sleep` w taki sposób, żeby zapytania nie wychodziły zbyt często\n",
    "- używanie proxy (minus: adresy publicznych proxy są powszechnie znane i często pojawiają się na blacklistach portali)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
